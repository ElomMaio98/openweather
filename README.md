# ğŸŒ¦ï¸ Weather Data Pipeline - Real-Time Streaming

Pipeline de dados em tempo real para coleta, processamento e visualizaÃ§Ã£o de dados meteorolÃ³gicos das principais capitais brasileiras.

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![Kafka](https://img.shields.io/badge/Kafka-Confluent-black.svg)](https://www.confluent.io/)
[![TimescaleDB](https://img.shields.io/badge/Database-TimescaleDB-orange.svg)](https://www.timescale.com/)
[![Grafana](https://img.shields.io/badge/Visualization-Grafana-orange.svg)](https://grafana.com/)
[![Railway](https://img.shields.io/badge/Deploy-Railway-purple.svg)](https://railway.app/)

---

## ğŸ“‹ Ãndice

- [Sobre o Projeto](#sobre-o-projeto)
- [Arquitetura](#arquitetura)
- [Tecnologias](#tecnologias)
- [PrÃ©-requisitos](#prÃ©-requisitos)
- [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
- [ConfiguraÃ§Ã£o](#configuraÃ§Ã£o)
- [Como Executar](#como-executar)
- [Deploy](#deploy)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Dashboards](#dashboards)
- [Troubleshooting](#troubleshooting)
- [Roadmap](#roadmap)
- [Contribuindo](#contribuindo)
- [LicenÃ§a](#licenÃ§a)

---

## ğŸ¯ Sobre o Projeto

Este projeto implementa um **pipeline de dados em tempo real** para:

- ğŸ“¡ Coletar dados meteorolÃ³gicos de 27 capitais brasileiras via OpenWeatherMap API
- ğŸ“¨ Processar dados atravÃ©s de message broker (Apache Kafka)
- ğŸ’¾ Armazenar em banco de dados otimizado para sÃ©ries temporais (TimescaleDB)
- ğŸ“Š Visualizar em dashboards interativos (Grafana)

### âœ¨ Funcionalidades

- âœ… Coleta automatizada a cada 5-10 minutos
- âœ… Processamento assÃ­ncrono com Kafka
- âœ… Armazenamento eficiente de time-series
- âœ… Dashboards em tempo real
- âœ… Filtros por cidade
- âœ… VisualizaÃ§Ã£o geogrÃ¡fica (mapas)
- âœ… CÃ¡lculo de mÃ©dias e estatÃ­sticas
- âœ… Deploy cloud-native

---

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              â”‚      â”‚              â”‚      â”‚              â”‚      â”‚              â”‚
â”‚ OpenWeather  â”‚â”€â”€â”€â”€â”€â–¶â”‚   Producer   â”‚â”€â”€â”€â”€â”€â–¶â”‚    Kafka     â”‚â”€â”€â”€â”€â”€â–¶â”‚   Consumer   â”‚
â”‚     API      â”‚      â”‚   (Python)   â”‚      â”‚  (Confluent) â”‚      â”‚   (Python)   â”‚
â”‚              â”‚      â”‚              â”‚      â”‚              â”‚      â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                          â”‚
                                                                          â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚              â”‚                           â”‚              â”‚
                      â”‚   Grafana    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  TimescaleDB â”‚
                      â”‚  Dashboards  â”‚                           â”‚  (Supabase)  â”‚
                      â”‚              â”‚                           â”‚              â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados

1. **Producer** busca dados da API OpenWeatherMap (temperatura, umidade, pressÃ£o, vento)
2. **Kafka** recebe e armazena mensagens no tÃ³pico `dados_brutos`
3. **Consumer** processa mensagens e persiste no banco
4. **TimescaleDB** armazena dados otimizados para queries temporais
5. **Grafana** visualiza dados em tempo real

---

## ğŸ› ï¸ Tecnologias

### Backend & Processing
- **Python 3.11** - Linguagem principal
- **Apache Kafka** - Message broker (Confluent Cloud)
- **kafka-python** - Client Kafka

### Database
- **Supabase** - PostgreSQL gerenciado
- **TimescaleDB** - ExtensÃ£o para time-series
- **PostGIS** - Dados geoespaciais

### Visualization
- **Grafana** - Dashboards e grÃ¡ficos

### Infrastructure
- **Railway** - Deploy e hospedagem
- **Docker** - ContainerizaÃ§Ã£o
- **Git** - Controle de versÃ£o

### APIs Externas
- **OpenWeatherMap API** - Dados meteorolÃ³gicos

---

## âš™ï¸ PrÃ©-requisitos

### Contas e Credenciais

1. **OpenWeatherMap API**
   - Criar conta em [openweathermap.org](https://openweathermap.org/)
   - Gerar API Key gratuita

2. **Confluent Cloud** (Kafka)
   - Criar conta em [confluent.cloud](https://confluent.cloud/)
   - Criar cluster Kafka
   - Criar API Key e Secret

3. **Supabase**
   - Criar projeto em [supabase.com](https://supabase.com/)
   - Obter URL e API Key

4. **Railway** (para deploy)
   - Criar conta em [railway.app](https://railway.app/)
   - Conectar com GitHub

### Desenvolvimento Local

- Python 3.11+
- pip ou poetry
- Git
- Docker (opcional)

---

## ğŸ“¦ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/weather-data-pipeline.git
cd weather-data-pipeline
```

### 2. Crie ambiente virtual

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

### 3. Instale dependÃªncias

```bash
pip install -r requirements.txt
```

### 4. Configure variÃ¡veis de ambiente

Crie arquivo `.env` na raiz:

```bash
cp .env.example .env
```

Edite `.env` com suas credenciais (veja seÃ§Ã£o [ConfiguraÃ§Ã£o](#configuraÃ§Ã£o))

---

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

#### **OpenWeatherMap API**
```bash
WEATHER_KEY=sua_api_key_aqui
```

#### **Kafka (Confluent Cloud)**
```bash
KAFKA_BOOTSTRAP_SERVERS=pkc-xxxxx.us-east-1.aws.confluent.cloud:9092
KAFKA_SASL_USERNAME=seu_username
KAFKA_SASL_PASSWORD=seu_password
KAFKA_TOPIC=dados_brutos
KAFKA_CONSUMER_GROUP=weather-consumer-group
```

#### **Supabase**
```bash
SUPABASE_URL=https://seu-projeto.supabase.co
SUPABASE_KEY=sua_service_role_key
```

#### **ConfiguraÃ§Ãµes do Producer**
```bash
PRODUCER_SLEEP_SECONDS=300  # 5 minutos (padrÃ£o)
```

### Configurar Banco de Dados

Execute no SQL Editor do Supabase:

```sql
-- Criar tabela
CREATE TABLE weather_metrics (
    id BIGSERIAL PRIMARY KEY,
    city VARCHAR(100) NOT NULL,
    latitude FLOAT8,
    longitude FLOAT8,
    temperatura_celsius FLOAT4,
    humidity FLOAT4,
    pressure FLOAT4,
    wind_speed FLOAT4,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Criar Ã­ndices
CREATE INDEX idx_weather_time ON weather_metrics(created_at DESC);
CREATE INDEX idx_weather_city ON weather_metrics(city);

-- Habilitar TimescaleDB (opcional, mas recomendado)
CREATE EXTENSION IF NOT EXISTS timescaledb;
SELECT create_hypertable('weather_metrics', 'created_at', 
    chunk_time_interval => INTERVAL '7 days');
```

### Configurar LocalizaÃ§Ãµes

Edite `config/locations.json`:

```json
[
  {
    "name": "SÃ£o Paulo",
    "lat": -23.5505,
    "lon": -46.6333
  },
  {
    "name": "Rio de Janeiro",
    "lat": -22.9068,
    "lon": -43.1729
  }
]
```

---

## ğŸš€ Como Executar

### Modo Desenvolvimento (Local)

#### 1. Executar Producer

```bash
python producer/main.py
```

Logs esperados:
```
INFO - Iniciando Weather Data Producer
INFO - Intervalo de coleta: 300 segundos
INFO - 27 localizaÃ§Ãµes carregadas
âœ… SÃ£o Paulo: 25.3Â°C (lat: -23.5505, lon: -46.6333)
```

#### 2. Executar Consumer

Em outro terminal:

```bash
python consumer/main.py
```

Logs esperados:
```
INFO - ğŸš€ Iniciando Weather Consumer...
INFO - âœ… Consumer e Supabase conectados
INFO - ğŸ“¡ Aguardando mensagens do tÃ³pico 'dados_brutos'...
INFO - ğŸ“© Mensagem #1: SÃ£o Paulo - 25.3Â°C
INFO - âœ… Dados salvos no Supabase
```

### Modo ProduÃ§Ã£o (Docker)

#### Producer
```bash
docker build -t weather-producer ./producer
docker run --env-file .env weather-producer
```

#### Consumer
```bash
docker build -t weather-consumer ./consumer
docker run --env-file .env weather-consumer
```

---

## â˜ï¸ Deploy

### Railway

#### 1. Criar projeto no Railway

```bash
railway login
railway init
```

#### 2. Deploy Producer

```bash
railway up --service producer
```

#### 3. Deploy Consumer

```bash
railway up --service consumer
```

#### 4. Configurar variÃ¡veis de ambiente

No Railway Dashboard:
- Selecione cada serviÃ§o (producer/consumer)
- VÃ¡ em "Variables"
- Adicione todas as variÃ¡veis do `.env`

#### 5. Verificar logs

```bash
railway logs --service producer
railway logs --service consumer
```

---

## ğŸ“ Estrutura do Projeto

```
weather-data-pipeline/
â”‚
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ main.py              # Script principal do producer
â”‚   â”œâ”€â”€ Dockerfile           # Container do producer
â”‚   â””â”€â”€ requirements.txt     # DependÃªncias do producer
â”‚
â”œâ”€â”€ consumer/
â”‚   â”œâ”€â”€ main.py              # Script principal do consumer
â”‚   â”œâ”€â”€ Dockerfile           # Container do consumer
â”‚   â””â”€â”€ requirements.txt     # DependÃªncias do consumer
â”‚
â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ apiclient.py         # Client da OpenWeatherMap API
â”‚   â”œâ”€â”€ kafkaclient.py       # Client do Kafka
â”‚   â””â”€â”€ supabaseclient.py    # Client do Supabase
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ locations.json       # Lista de cidades para monitorar
â”‚
â”œâ”€â”€ .env.example             # Template de variÃ¡veis de ambiente
â”œâ”€â”€ .gitignore              # Arquivos ignorados pelo Git
â”œâ”€â”€ README.md               # Este arquivo
â””â”€â”€ requirements.txt        # DependÃªncias globais
```

---

## ğŸ“Š Dashboards

### Grafana - Como Conectar

1. **Adicionar Data Source**
   - Type: PostgreSQL
   - Host: `seu-projeto.supabase.co:5432`
   - Database: `postgres`
   - User: `postgres`
   - Password: (sua senha do Supabase)
   - SSL Mode: `require`

2. **Importar Dashboards**

Crie painÃ©is com estas queries:

#### Temperatura ao longo do tempo
```sql
SELECT 
  created_at as time,
  temperatura_celsius
FROM weather_metrics
WHERE city = '${Cidade}'
  AND $__timeFilter(created_at)
ORDER BY time
```

#### MÃ©dia de temperatura
```sql
SELECT 
  AVG(temperatura_celsius) as "Temperatura MÃ©dia"
FROM weather_metrics
WHERE city = '${Cidade}'
  AND $__timeFilter(created_at)
```

#### Mapa geogrÃ¡fico
```sql
SELECT DISTINCT ON (city)
  created_at as time,
  city,
  latitude,
  longitude,
  temperatura_celsius
FROM weather_metrics
WHERE $__timeFilter(created_at)
ORDER BY city, created_at DESC
```

### VariÃ¡vel Dropdown (Filtro de Cidade)

**Name:** `Cidade`
**Type:** Query
**Query:**
```sql
SELECT DISTINCT city FROM weather_metrics ORDER BY city
```

---

## ğŸ› Troubleshooting

### Producer nÃ£o envia mensagens

**Problema:** Erro de autenticaÃ§Ã£o no Kafka

**SoluÃ§Ã£o:**
```bash
# Verificar credenciais do Kafka
echo $KAFKA_SASL_USERNAME
echo $KAFKA_SASL_PASSWORD

# Testar conectividade
telnet seu-broker.confluent.cloud 9092
```

### Consumer nÃ£o processa mensagens

**Problema:** Offset do consumer estÃ¡ no final

**SoluÃ§Ã£o:**
```python
# Em kafkaclient.py, adicionar:
'auto_offset_reset': 'earliest'  # Ler desde o inÃ­cio
```

### Dados NULL no banco

**Problema:** Campo latitude/longitude NULL

**SoluÃ§Ã£o:**
- Verificar se `locations.json` tem campos `lat` e `lon`
- Verificar logs do producer para confirmar coleta
- Verificar se tabela tem colunas `latitude` e `longitude`

### GrÃ¡fico serrilhado no Grafana

**Problema:** Muitos pontos de dados

**SoluÃ§Ã£o:**
```sql
-- Usar time_bucket para agregar
SELECT 
  time_bucket('1 hour', created_at) AS time,
  AVG(temperatura_celsius) as temp
FROM weather_metrics
WHERE $__timeFilter(created_at)
GROUP BY time_bucket('1 hour', created_at)
ORDER BY time
```

### Logs de Debug

Adicionar ao cÃ³digo:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

## ğŸ—ºï¸ Roadmap

### VersÃ£o 1.0 (Atual)
- [x] Producer para coleta de dados
- [x] Consumer para persistÃªncia
- [x] Dashboards bÃ¡sicos no Grafana
- [x] Deploy no Railway

### VersÃ£o 2.0 (PrÃ³xima)
- [ ] Adicionar mais campos (UV index, sensaÃ§Ã£o tÃ©rmica)
- [ ] Implementar alertas (temperatura > 35Â°C)
- [ ] API REST para consultar histÃ³rico
- [ ] Testes unitÃ¡rios e de integraÃ§Ã£o

### VersÃ£o 3.0 (Futuro)
- [ ] Machine Learning para previsÃ£o
- [ ] Data Quality checks
- [ ] Migrar para Avro (em vez de JSON)
- [ ] Implementar CDC (Change Data Capture)
- [ ] Multiple consumers para diferentes processamentos

---

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Siga estes passos:

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/MinhaFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona MinhaFeature'`)
4. Push para a branch (`git push origin feature/MinhaFeature`)
5. Abra um Pull Request

### Diretrizes

- Siga PEP 8 para cÃ³digo Python
- Adicione docstrings em funÃ§Ãµes
- Mantenha commits pequenos e descritivos
- Teste localmente antes de criar PR

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---

## ğŸ‘¨â€ğŸ’» Autor

**Seu Nome**
- GitHub: [@ElomMaio98](https://github.com/ElomMaio98)
- LinkedIn: [elom-maio](https://www.linkedin.com/in/elom-maio)
- Medium: [@seu-usuario](https://medium.com/@seu-usuario)

---

## ğŸ™ Agradecimentos

- [OpenWeatherMap](https://openweathermap.org/) - API de dados meteorolÃ³gicos
- [Confluent](https://www.confluent.io/) - Kafka gerenciado
- [Supabase](https://supabase.com/) - PostgreSQL gerenciado
- [Grafana](https://grafana.com/) - Plataforma de visualizaÃ§Ã£o
- [Railway](https://railway.app/) - Deploy simplificado

---

## ğŸ“š Recursos Adicionais

- [DocumentaÃ§Ã£o Kafka](https://kafka.apache.org/documentation/)
- [TimescaleDB Docs](https://docs.timescale.com/)
- [Grafana Tutorials](https://grafana.com/tutorials/)
- [Artigo no Medium](link-do-seu-artigo) - ExplicaÃ§Ã£o detalhada do projeto

---

**â­ Se este projeto foi Ãºtil, considere dar uma estrela!**